---
layout: post
title: How to become a data engineer (Data Engineer Roadmap)
# author: "Himanshu Aggarwal"
# description: "How to become a data engineer"
date: 2022-03-27
categories: data engineer code wall
---
<br>
<center><figure>
<img src="{{ site.url }}/images/footstep_all.png"/>
<figcaption align = "center"><i>Data Learning Journey Footsteps</i></figcaption>
</figure></center>


With increasing internet activity, the amount of data being generated is increasing at an enormous rate from the past decade, especially in the last few years, it is huge about 2.5 quintillion (2.5 e+9GB) per day.
Companies are thriving to extract value from the generated data, building and maintaining the system and architecture in place to handle this enormous big data falls under the responsibility of a data engineer. So, we can say that data engineers are here to stay in the market and the same has already been in the
[Top Demanding Skill](https://analyticsindiamag.com/are-data-engineering-jobs-getting-more-popular-than-data-science-jobs/#:~:text=In%20High%20Demand,over%20the%20past%20twelve%20months).

In this article, we have prepared a roadmap on how to become a data engineer. <br>
This data engineer roadmap is applicable to everyone, whether you are at the beginner or advanced level.<br>
Let's begin with the skills and path required to become a data engineer.Before moving to data engineer-specific skills, it is a must to have computer science fundamentals.<br>
We will be covering the overall data engineer career journey in the 7 footsteps each being tagged as beginner/advance mentioned above.
<br>We have prepared a mindmap for the same.<br>

<center>
<figure>
<iframe style="border: none;" src="https://whimsical.com/embed/FY2YFMaatyVizX954DjnN" width="800" height="450"></iframe>
<figcaption> <a href="https://whimsical.com/embed/FY2YFMaatyVizX954DjnN">Data Engineer Skills Mindmap</a> <i>(zoom in (+) to browse)</i></figcaption>
</figure>
</center>


Each footstep will have three sections mentioned below :
- Basic Idea and Topics – Will give you a basic idea of the topics that need to be covered.
- Resources from where the same can be learned, these resources also helped a lot in my own data engineering journey.
<br><br>

## Footstep 1:  Database fundamentals and learning SQL

Whenever there is a need to store the data (specifically structured data) database plays an important role as a storage layer and SQL(sequel) as a language tool to interact with the database.<br>
So, it becomes very important to have strong database fundamentals and SQL concepts when dealing with data.<br>
The basic idea on the topics to be learned :

<center>
<figure>
<img src="{{ site.url }}/images/footstep_1_sql.png"/>
<figcaption align = "center"><i>SQL Learning</i></figcaption>
</figure>
</center>

Resources
- Database Fundamentals
- SQL (Sequel): [Youtube Full course video](https://www.youtube.com/watch?v=HXV3zeQKqGY\), [W3 Schools](https://www.w3schools.com/sql/)
- Practice Set: [HackerRank SQL](https://www.hackerrank.com/domains/sql)  
- Recommended Time Duration: 1 week (1-1.5 hr/day)
<br><br>

## Footstep 2 : Programming Language

Learning a programming language is a must. If you come from a computer science background, then you will already have a basic understanding of the same.
<br>
Building this solid foundation will help you in reducing your learning curve because programming language will be needed to interact with several services.<br>
There are three options of programming language with preference order(Python >> Scala >> Java) that are widely used across the data engineering industry.<br>
If you are just starting your journey and new to all of the three programming languages mentioned above, our recommendation would be to learn python (as it is widely adopted in the data field).<br>

*Resources :* <br>
- Learning : [Python Series](https://www.youtube.com/watch?v=eXBD2bB9-RA&amp;list=PLQVvvaa0QuDeAams7fkdcwOGBpGdHpXln), [W3 schools](https://www.w3schools.com/python/)
- Practice Set : [HackerRank Python](https://www.hackerrank.com/domains/python) , [Tutorial plus HackerRank Python Solutions(Our Guide)](https://github.com/himanshu-aggarwal0909/python-adventures) <br>
You can check out one of our articles on [python quick reference guide]("https://datathrust.in/python-programming-language-quick-reference-guide-beginner-guide/), to have a basic python guide handy.

*Recommended Time Duration :* 1-2 Week (1.5 hr/day)
<br><br>

## Footstep 3: Data Structures and Algorithms(DSA)
<br>
Data Structures and algorithms(DSA) make the core of the computer science stream. It is the basic building block of algorithmic thinking, and logical capabilities in any field of computer science and data engineering is no exception. It improves the processing power of the system by making efficient use of the stored data. Learning any programming language is okay but using it efficiently is only possible by DSA.<br> 
For a data engineer, you only need to have a basic understanding of DSA, no need to deep dive deeper or into competitive coding to become a data engineer (though it is always better to have a strong grip on DSA).<br>

Topics that should be covered : 
- Array
- Linked List
- Stack
- Queue
- Binary Tree
- Binary Search Tree (BST)
- Heap
- Hashing
- Graph
- Advanced DSA (optional for a data engineer, but always better to have)

*Resources :*<br>
- Learning: [Geeks for Geeks DSA](https://www.geeksforgeeks.org/data-structures/)
- Practice Set: [HackerRank](https://www.hackerrank.com/domains/data-structures) <br>
If you are preparing for the interview then, would recommend [cracking the coding interview book](https://www.amazon.in/Cracking-the-Coding-Interview/dp/0984782869/ref=pd_sbs_1/262-8742333-8808711?pd_rd_w=lktqI&amp;pf_rd_p=18688541-e961-44b9-b86a-bd9b8fa83027&amp;pf_rd_r=J0WKVVRNJN5G924EFFGN&amp;pd_rd_r=6e6057cb-bfc5-478a-9461-43745e4f0346&amp;pd_rd_wg=R4B72&amp;pd_rd_i=0984782869&amp;psc=1) and [leet code for practice](https://leetcode.com/problemset/all).

This may seem overwhelming but make sure to always stick to the basics in DSA and expand your knowledge by solving problems at every day/week level from the resources above (as per the suitability).<br>
Bonus, this [Github Repo](https://github.com/jainaman224/Algo_Ds_Notes) has all the data structures and algorithms implemented in every language (can help you a lot during your DSA journey).

*Recommended Time Duration :* 2 weeks + ongoing learning.
<br><br>

## Footstep 4: Big Data Concepts and framework

<center>
<figure>
<img src="{{ site.url }}/images/footstep_4_bigdata_concepts.png"/>
<figcaption align = "center"><i>Big Data Learning (Concepts + Tools)</i></figcaption>
</figure>
</center>

Big Data is itself a blanket term, it's difficult to find an exact definition for the same because projects and industry practitioners use it differently. In general, we can say that <strong>big data includes the computing strategies and technologies to handle large datasets</strong>.<br>
Here, large datasets represent the data that cannot be stored or process on a single computer or traditional technology system.<br>
In order to process this much amount of data, we need to learn about big data tools.
In the above picture, we have specified the approach to learning the big data along with the concept-wise topics that you should definitely hunt for while learning that concept. Apache Hadoop and apache spark are the two main and popular frameworks for processing big data.<br />You can see Hadoop as an ecosystem, having a large number of toolsets for better processing and managing big data like <br> 
MapReduce - Basic framework that is used behind distributed computing.<br>
HDFS (Hadoop distributed file system) - the distributed data storage layer.<br>
At this time, it may seem overwhelming, but remember a data engineer grabs all these concepts over a year.<br>
So, sit tight and start your journey in big data with enthusiasm.


*Resources :*<br>
- Courses: [Coursera Speciality on Big data (Paid)](https://www.coursera.org/learn/big-data-essentials), [Udemy course on Big data(Paid)](https://www.udemy.com/course/the-ultimate-hands-on-hadoop-tame-your-big-data/), Edureka
- Books for data engineering tools: [Hadoop The Definitive Guide](https://www.amazon.in/Hadoop-Definitive-Guide-Tom-White/dp/9352130677/ref=sr_1_1?dchild=1&amp;keywords=hadoop+definitive+guide&amp;qid=1627823722&amp;sr=8-1), [Learning Spark](https://www.oreilly.com/library/view/learning-spark/9781449359034/)

The above-mentioned Coursera course is the paid one but still, you can apply for financial aid as well, if needed. Usually, they are generous enough to approve your aid, if found genuine.<br>
You can also find a number of videos on youtube as well but a structured course would be much better (especially for this much larger stack) to make the foundation in a structured way and strong enough.<br>

*Recommended Time Duration :* 1.5-2 Months (2-3 hr/day)
<br><br>

## Footstep 5 : Data Orchestration / Workflow management

Big Data processing is majorly performed in three phases <strong>E</strong>xtract, <strong>T</strong>ransform and <strong>L</strong>oad, and the data is collected from a large variety of sources, processed in multiple steps.<br>
The automation of these data-driven processes from end to end involving collection, preparing data, and making decisions based on the processed data is known as data orchestration.<br>
There are multiple tools to programmatically handle, author, and manage these data pipelines, this process comes under workflow management.

<center>
<figure>
<img src="{{ site.url }}/images/footstep_5_data_workflow_mngmnt.png"/>
<figcaption align = "center"><i>Workflow Management</i></figcaption>
</figure>
</center>

Mostly, you will grab full knowledge about workflow management while working professionally or handling a big data pipeline project.<br>
If you are starting from scratch in workflow management (either relying on cron schedulers before) then, Airflow would be the best option to play with as it is most widely used across the journey

*Resources :* <br> 
- [Youtube Apache Airflow](https://www.youtube.com/playlist?list=PLYizQ5FvN6pvIOcOd6dFZu3lQqc6zBGp2), [Azkaban](https://azkaban.github.io/)

*Recommended Time Duration :* 2 Days (3-4 hr/day)
<br>

## Footstep 6: Cloud Services

Open-source frameworks (like Hadoop and spark) have contributed a lot in setting up the big data ecosystem.<br>
But having a need to turn up large clusters of commodity hardware (computers) to process big data have some constraints like 
- Maintaining large on-premise clusters.
- Space and specialized staff to manage the same.
- Scalability (during peak or less demand)
- High Cost.

Here, the cloud services (provided by large cloud providers like Amazon Web Services (AWS), Microsoft Azure, and Google Cloud Services (GCP)) have enabled the Big Data ecosystem in yielding scalable and cost-effective solutions.<br>
It is also a kind of overwhelming or confusing to where to start learning from the plethora of cloud services provided in the market.So, we have mentioned below the services that are provided by Amazon Web Services(AWS) along with their domain usage.<br>
It gives a basic idea of what services to catch up on while learning the AWS cloud services applicable to the big data domain as well.<br>

<center>
<figure>
<img src="{{ site.url }}/images/footstep_6_cloud_service_aws.png"/>
<figcaption align = "center"><i>AWS Cloud services - Data Engineer Cloud Skills Path</i></figcaption>
</figure>
</center>

*Resources :* <br> 
- [Youtube AWS Classes](https://www.youtube.com/c/amazonwebservices/search?query=data)
- [Youtube AWS Online tech talks](https://www.youtube.com/channel/UCT-nPlVzJI-ccQXlxjSvJmw)
- [Edureka](https://www.youtube.com/watch?v=k1RI5locZE4)

*Recommended Time Duration :* 1 month (2 hours/day)
<br>

## Footstep 7 : Real Time Streaming

In the end, the reason behind any data processing is to extract value from it, and in this time there are many use cases where we need to make decisions/extract the value as the data is generated like providing real-time recommendations to users, real-time stock trading or real-time fraud detection and many more.<br>
The process of analyzing a large amount of data as it is produced is known as real time streaming.When we combine batch processing (where the data analysis is not needed in real time) and real time streaming in a single platform, it makes up the hybrid data platform.<br> 
There are many tools in the market to provide real time streaming. We have listed some of the most adopted frameworks/services below.<br>

<center>
<figure>
<img src="{{ site.url }}/images/footstep_7_real_time_streaming.png"/>
<figcaption align = "center"><i>Real Time Streaming - Data Engineer Skills Path</i></figcaption>
</figure>
</center>

If you are confused and completely new to real streaming then, start with Apache Kafka + Apache spark streaming (the combination of both is adopted heavily in the market) and will mostly cover all the aspects of real time streaming.

*Resources :* <br>
- [Intellipath](https://www.youtube.com/watch?v=daRykH67_qs&amp;t=0s)
- [Udacity Data Streaming Nanodegree(paid)](https://www.udacity.com/course/data-streaming-nanodegree--nd029) 
- [Linkedin Learning Apache Kafka](https://www.linkedin.com/learning/learn-apache-kafka-for-beginners)

*Recommended Time Duration :* 2 months (5-10 hrs/week)
<br>
Here, comes an end to this roadmap but remember learning is a process that should never end.<br>
Give yourself room and time to grow. <br>

~~ Good Luck!!